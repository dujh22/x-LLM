# å¯¼å…¥å¿…è¦çš„åº“
import torch  # torchæ˜¯ä¸€ä¸ªå¼€æºçš„æœºå™¨å­¦ä¹ åº“ï¼Œç”¨äºæ·±åº¦å­¦ä¹ å’Œå¼ é‡è®¡ç®—
import transformers  # transformersæ˜¯ä¸€ä¸ªå¼€æºçš„è‡ªç„¶è¯­è¨€å¤„ç†åº“ï¼Œæä¾›å¤§é‡é¢„è®­ç»ƒæ¨¡å‹
import gradio as gr  # gradioæ˜¯ä¸€ä¸ªåº“ï¼Œç”¨äºå¿«é€Ÿåˆ›å»ºæœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦çš„Web UI
import sys  # sysæ¨¡å—æä¾›äº†ä¸€ç³»åˆ—æœ‰å…³Pythonè¿è¡Œç¯å¢ƒçš„å˜é‡å’Œå‡½æ•°

# ä»train.pyä¸­å¯¼å…¥ç‰¹å®šçš„å‡½æ•°å’Œå˜é‡
from train import smart_tokenizer_and_embedding_resize, \
    DEFAULT_PAD_TOKEN, DEFAULT_EOS_TOKEN, DEFAULT_BOS_TOKEN, DEFAULT_UNK_TOKEN, \
    PROMPT_DICT, \
    DataArguments
# è¿™äº›å¯¼å…¥é¡¹åŒ…æ‹¬ç”¨äºè°ƒæ•´tokenizerå’Œembeddingå¤§å°çš„å‡½æ•°ã€é»˜è®¤çš„ç‰¹æ®Štokenã€ç”¨äºå­˜å‚¨promptæ¨¡æ¿çš„å­—å…¸ã€ä»¥åŠä¸€ä¸ªç”¨äºå­˜å‚¨æ•°æ®å‚æ•°çš„ç±»

# è·å–å‘½ä»¤è¡Œå‚æ•°ä¸­æŒ‡å®šçš„åŸºç¡€æ¨¡å‹åç§°ï¼Œè®¾ç½®é»˜è®¤å€¼ä¸ºmodel/llama-7b-hf.alpaca_en+alpaca_zh+translation_ncwm_en-zh.finetune
BASE_MODEL = "model/llama-7b-hf.alpaca_en+alpaca_zh+translation_ncwm_en-zh.finetune"
if len(sys.argv) > 1:
    BASE_MODEL = sys.argv[1]  # ç¬¬ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°è¢«è§†ä¸ºåŸºç¡€æ¨¡å‹çš„åç§°


# æ ¹æ®ç³»ç»Ÿæ˜¯å¦æ”¯æŒCUDAå†³å®šä½¿ç”¨çš„è®¾å¤‡
if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

# å°è¯•æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æ”¯æŒMPSåç«¯ï¼ŒMPSæ˜¯ç”¨äºMacç³»ç»Ÿä¸Šçš„GPUåŠ é€Ÿ
try:
    if torch.backends.mps.is_available():
        device = "mps"
except:
    pass  # å¦‚æœæ£€æŸ¥æ—¶å‘ç”Ÿå¼‚å¸¸ï¼Œåˆ™å¿½ç•¥è¯¥å¼‚å¸¸

# æ ¹æ®è®¾å¤‡ç±»å‹åŠ è½½æ¨¡å‹
if device == "cuda":
    model = transformers.AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        load_in_8bit=False,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
elif device == "mps":
    model = transformers.AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map={"": device},
        torch_dtype=torch.bfloat16,
    )
else:
    model = transformers.AutoModelForCausalLM.from_pretrained(
        BASE_MODEL, device_map={"": device}, low_cpu_mem_usage=True
    )

# åŠ è½½tokenizer
tokenizer = transformers.AutoTokenizer.from_pretrained(
    BASE_MODEL,
    use_fast=False,
)

# åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸ç”¨äºå­˜å‚¨ç‰¹æ®Štoken
special_tokens_dict = dict()
# æ£€æŸ¥å¹¶è®¾ç½®tokenizerçš„ç‰¹æ®Štoken
if tokenizer.pad_token is None:
    special_tokens_dict["pad_token"] = DEFAULT_PAD_TOKEN
if tokenizer.eos_token is None:
    special_tokens_dict["eos_token"] = DEFAULT_EOS_TOKEN
if tokenizer.bos_token is None:
    special_tokens_dict["bos_token"] = DEFAULT_BOS_TOKEN
if tokenizer.unk_token is None:
    special_tokens_dict["unk_token"] = DEFAULT_UNK_TOKEN

# å¦‚æœéœ€è¦ï¼Œè°ƒæ•´tokenizerå’Œæ¨¡å‹çš„embedding
if tokenizer.pad_token is None:
    smart_tokenizer_and_embedding_resize(
        special_tokens_dict=special_tokens_dict,
        tokenizer=tokenizer,
        model=model,
    )
# è®¾ç½®tokenizerçš„paddingæ–¹å‘
tokenizer.padding_side = "left"

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨äºç”Ÿæˆprompt
def generate_prompt(instruction, input=None, template="alpaca"):
    # æ ¹æ®æ¨¡æ¿ç±»å‹ç”Ÿæˆä¸åŒçš„prompt
    if template == "alpaca":
        if input:
            return PROMPT_DICT["prompt_input"].format(instruction=instruction, input=input)
        else:
            return PROMPT_DICT["prompt_no_input"].format(instruction=instruction)
    elif template == "raw":
        if input:
            return f"{instruction}\n\n{input}"
        else:
            return f"{instruction}"
    else:
        raise NotImplementedError

# å¦‚æœä½¿ç”¨çš„ä¸æ˜¯CPUè®¾å¤‡ï¼Œå°†æ¨¡å‹è½¬ä¸ºåŠç²¾åº¦ä»¥èŠ‚çœå†…å­˜
if device != "cpu":
    model.half()
model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
if torch.__version__ >= "2":
    model = torch.compile(model)  # å¦‚æœPyTorchç‰ˆæœ¬å¤§äºç­‰äº2ï¼Œå¯¹æ¨¡å‹è¿›è¡Œç¼–è¯‘ä»¥ä¼˜åŒ–æ€§èƒ½

# å®šä¹‰è¯„ä¼°å‡½æ•°
def evaluate(
    instruction,  # æŒ‡ä»¤
    input=None,  # è¾“å…¥
    template="alpaca", # ä½¿ç”¨çš„æ¨¡æ¿ç±»å‹
    temperature=0.1, # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§çš„æ¸©åº¦å‚æ•°
    top_p=0.75, # åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œä¿ç•™ç´¯ç§¯æ¦‚ç‡ä¸ºtop_pçš„æœ€å¯èƒ½çš„è¯
    top_k=40, # åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œä¿ç•™æ¦‚ç‡æœ€é«˜çš„top_kä¸ªè¯
    num_beams=4, # beamæœç´¢çš„æ•°é‡ï¼Œç”¨äºç”Ÿæˆæ›´å¥½çš„æ–‡æœ¬è¾“å‡º
    max_new_tokens=128, # ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦
    **kwargs, # å…¶ä»–é…ç½®å‚æ•°
):
    # ç”Ÿæˆprompt
    prompt = generate_prompt(instruction, input, template)
    # å¯¹promptè¿›è¡Œtokenizeå¤„ç†
    inputs = tokenizer(prompt, return_tensors="pt")
    # å°†tokenizedçš„è¾“å…¥è½¬ç§»åˆ°æŒ‡å®šè®¾å¤‡
    input_ids = inputs["input_ids"].to(device)
    # è®¾ç½®ç”Ÿæˆæ–‡æœ¬çš„é…ç½®å‚æ•°
    generation_config = transformers.GenerationConfig(
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        num_beams=num_beams,
        **kwargs,
    )
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            return_dict_in_generate=True,
            output_scores=True,
            max_new_tokens=max_new_tokens,
        )
    # è·å–ç”Ÿæˆçš„åºåˆ—
    s = generation_output.sequences[0]
    # å°†ç”Ÿæˆçš„åºåˆ—è§£ç æˆæ–‡æœ¬
    output = tokenizer.decode(s, skip_special_tokens=True)
    # å»é™¤promptéƒ¨åˆ†ï¼Œåªè¿”å›ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
    return output[len(prompt):].strip()

# åˆ›å»ºä¸€ä¸ªgradioç•Œé¢
g = gr.Interface(
    # è®¾ç½®ç•Œé¢è°ƒç”¨çš„å‡½æ•°
    fn=evaluate,
    # è®¾ç½®ç•Œé¢çš„è¾“å…¥ç»„ä»¶
    inputs=[
        gr.components.Textbox(
            lines=2, label="Instruction", placeholder="Tell me about alpacas."
        ),
        gr.components.Textbox(lines=2, label="Input", placeholder="none"),
        gr.components.Dropdown(["raw", "alpaca"], value="alpaca", label="Template Format"),
        gr.components.Slider(minimum=0, maximum=1, value=0.7, label="Temperature"),
        gr.components.Slider(minimum=0, maximum=1, value=0.75, label="Top p"),
        gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label="Top k"),
        gr.components.Slider(minimum=1, maximum=4, step=1, value=1, label="Beams"),
        gr.components.Slider(
            minimum=1, maximum=512, step=1, value=512, label="Max tokens"
        ),
    ],
    # è®¾ç½®ç•Œé¢çš„è¾“å‡ºç»„ä»¶
    outputs=[
        gr.inputs.Textbox(
            lines=5,
            label="Output",
        )
    ],
    # ç•Œé¢æ ‡é¢˜
    title="ğŸ¦™Alpaca Web Demo",
    # ç•Œé¢æè¿°
    description="Made with ğŸ’–",
    # æä¾›çš„åé¦ˆé€‰é¡¹
    flagging_options=["ğŸ‘ğŸ¼", "ğŸ‘ğŸ¼"]
)
# è®¾ç½®é˜Ÿåˆ—å¹¶å‘æ•°
g.queue(concurrency_count=1)
# å¯åŠ¨ç•Œé¢
g.launch(server_name='0.0.0.0', server_port=8086)

# Old testing code follows.

"""
if __name__ == "__main__":
    # testing code for readme
    for instruction in [
        "Tell me about alpacas.",
        "Tell me about the president of Mexico in 2019.",
        "Tell me about the king of France in 2019.",
        "List all Canadian provinces in alphabetical order.",
        "Write a Python program that prints the first 10 Fibonacci numbers.",
        "Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.",
        "Tell me five words that rhyme with 'shock'.",
        "Translate the sentence 'I have no mouth but I must scream' into Spanish.",
        "Count up from 1 to 500.",
    ]:
        print("Instruction:", instruction)
        print("Response:", evaluate(instruction))
        print()
"""
